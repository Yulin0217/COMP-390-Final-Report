\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{a4paper}
\usepackage[colorlinks, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage[automake]{glossaries-extra}
\usepackage{appendix}
\usepackage{graphicx} % Needed for including images
\usepackage{mdframed} % For creating framed boxes
\usepackage[backend=biber, style=ieee]{biblatex} % Adding biblatex with IEEE style
\usepackage{minted}

\addbibresource{Reference.bib} % Specify the bibliography file, here 'references.bib'


\makeglossaries % 初始化术语表系统

% 定义一些术语
 
\newglossaryentry{opencv}{
    name=OpenCV,
    description={OpenCV (Open Source Computer Vision Library) is a library of programming functions mainly for real-time computer vision. Originally developed by Intel, it was later supported by Willow Garage, then Itseez (which was later acquired by Intel). The library is cross-platform and licensed as free and open-source software under Apache License 2. Starting in 2011, OpenCV features GPU acceleration for real-time operations.\cite{qt}}
}

\newglossaryentry{QT}{
    name=Qt,
    description={Qt (pronounced "cute" or as an initialism) is cross-platform application development framework for creating graphical user interfaces as well as cross-platform applications that run on various software and hardware platforms such as Linux, Windows, macOS, Android or embedded systems with little or no change in the underlying codebase while still being a native application with native capabilities and speed.\cite{cv}}
}

\newglossaryentry{aruco}{
    name=Aruco Marker,
    description={ArUco markers are 2D binary-encoded fiducial patterns designed to be quickly located by computer vision systems. ArUco marker patterns are defined by a binary dictionary in OpenCV, and the various library functions return pattern IDs and pose information from scanned images.\cite{arucomarkers}}
}

\newglossaryentry{vtk}{
      name=VTK,
      description={The Visualization Toolkit (VTK) is an open-source, freely available software system for 3D computer graphics, modeling, image processing, volume rendering, scientific visualization, and 2D plotting. It supports a wide variety of visualization algorithms and advanced modeling techniques, and it takes advantage of both threaded and distributed memory parallel processing for speed and scalability, respectively.\cite{vtkBook}}
}

\newglossaryentry{alpha}{
      name=Alpha Blending,
      description={In computer graphics, alpha compositing or alpha blending is the process of combining one image with a background to create the appearance of partial or full transparency. It is often useful to render picture elements (pixels) in separate passes or layers and then combine the resulting 2D images into a single, final image called the composite. Compositing is used extensively in film when combining computer-rendered image elements with live footage. Alpha blending is also used in 2D computer graphics to put rasterized foreground elements over a background.\cite{Alpha}}
}

\newglossaryentry{aesl}{
      name=Adaptive Exponential Smoothing,
      description={Adaptive exponential smoothing models are designed to improve performance by letting the smoothing parameter vary according to the most recent forecasting accuracy. \cite{aes,aes2}}
}

\newglossaryentry{emal}{
      name=Exponential Moving Average,
      description={An exponential moving average (EMA) is a type of moving average (MA) that places a greater weight and significance on the most recent data points. The exponential moving average is also referred to as the exponentially weighted moving average. An exponentially weighted moving average reacts more significantly to recent price changes than a simple moving average simple moving average (SMA), which applies an equal weight to all observations in the period.\cite{InvestopediaEMA}}
}

\newglossaryentry{cesl}{
      name=Complex Exponential Smoothing,
      description={Complex exponential smoothing is a time series forecasting method that combines exponential smoothing with trend and seasonality. It is a variant of the standard exponential smoothing method, which is a simple technique for smoothing out data by using a weighted average of past observations.\cite{ComplexES2018}}
}





% 定义一个缩写词
\newabbreviation{aes}{AES}{
      Adaptive Exponential Smoothing}

\newabbreviation{ema}{EMA}{
      Exponential Moving Average}

\newabbreviation{ces}{CES}{
      Complex Exponential Smoothing}    


\begin{document}

\begin{titlepage}
      \centering

      \includegraphics[width=0.6\textwidth]{Liverpool.jpg} % Increased width
      \vspace*{1cm}

      \Large
      COMP390

      \large
      2023/24

      \vspace{0.5cm}
      \Huge
      \textbf{Computer Vision and AR for Endovascular Intervention}

      \vspace{1.5cm}


      % Framed box for student information
      \begin{mdframed}
            \normalsize % Smaller text size within the box
            \textbf{Student Name:} [Yulin Huang]\\[20pt] % Name on the same line, add vertical space
            \textbf{Student ID:} [201676465]\\[20pt] % ID on the same line, add vertical space
            \textbf{Supervisor Name:} [Anh Nguyen] % Supervisor on the same line
      \end{mdframed}

      \vspace{2cm} % Adjust space as necessary
      \Large
      \textbf{DEPARTMENT OF}\\
      \vspace{0.1cm} % Adjust line spacing
      \textbf{COMPUTER SCIENCE}

      \vspace{4cm} % Large space as required
      \large
      University of Liverpool\\
      Liverpool L69 3BX


\end{titlepage}
% Acknowledgements page
\newpage
\thispagestyle{empty} % Remove page number from the acknowledgements page
\begin{center}
      \Large \textbf{Acknowledgements}
\end{center}
\vspace{1cm}
\normalsize
%Here for the Acknowledgements

\newpage
\begin{titlepage}
      \centering


      \includegraphics[width=0.6\textwidth]{Liverpool.jpg} % Increased width
      \vspace*{1cm}

      \Large
      COMP390
      \large
      2023/24

      \vspace{4.5cm}
      \Huge
      \textbf{Computer Vision and AR for Endovascular Intervention}

      \vspace{1.5cm}



      \vspace{4cm} % Adjust space as necessary
      \Large
      \textbf{DEPARTMENT OF}\\
      \vspace{0.1cm} % Adjust line spacing
      \textbf{COMPUTER SCIENCE}

      \vspace{1cm} % Large space as required
      \large
      University of Liverpool\\
      Liverpool L69 3BX

\end{titlepage}
\tableofcontents
\newpage

% Abstract 
\section*{Abstract}
This section should contain a concise summary of the document content.
% Statement of Ethical Compliance
\section*{Statement of Ethical Compliance}
\begin{mdframed}
      \Large % Larger font size for the first two lines
      Data Category: A \\
      Participant Category: 0 \\
      \normalsize % Normal font size for the rest of the text
      I confirm that I have read the ethical guidelines and will follow them during this project. Further details can be found in the relevant sections of this proposal.
\end{mdframed}
\newpage

% Introduction & Background
\section{Introduction \& Background}



\section{Design \& Implementation}

\subsection{Part1: Real-world Model Interaction and Tracking}
% 这一部分集中讨论与现实世界模型的交互与跟踪的设计和实现。

In this part, I used \gls{opencv}\cite{opencv_library} and SciKit-Surgery Augmented Reality\cite{Thompson_SciKit-Surgery_Compact_Libraries_2020} libraries for image
processing and  model tracking. \gls{QT}\cite{QtWebsite} is used to design the graphical interface, and also \gls{vtk}\cite{vtkBook}. OpenCV and SciKit-Surgery libraries help me process images
and track \gls{aruco}\cite{1467495} within video steaming. Qt allows me to create a user interface that ensures  user could more easily change multiple settings, which can enhance the user experience.
VTK and SciKit-Surgery Augmented Reality library are the management of overlay and multi-layer video rendering in my project.
An ArUco Marker Generator is also included, which enables the user to generate different ArUco Markers and save them.
This section will detail the system's design, focusing on System Components and Organization, Data Structures and Algorithms,
User Interface Design, and Design Notation and Diagrams.


\subsubsection{Design}
\begin{enumerate}
      \item \textbf{System Components and Organization}
            \\The project is structured into three primary components, each responsible for distinct functionalities within the system.
            Here’s a detailed breakdown of these components and their organization:
            \begin{enumerate}
                  \item \textbf{Frontend - User Interface}
                        \\The system's user interface is developed using Qt, which is a framework that enables the creation of graphically applications.
                        The main class controlling the UI is \emph{Overlay\_and\_Tracking.py}, which serves as the central hub for user interactions and display functionalities.
                        This class manages the overlay of models on video streaming and provides interactive buttons for users to control various settings, such as model color,
                        video source, models uploading and changing, or adjusting ArUco marker types and sizes, and more.
                  \item \textbf{Backend - Helper Classes}
                        \\The backend is composed of various helper classes, each set to handle specific tasks:
                        \begin{itemize}
                              \item \textit{Image Capture:} The \emph{video\_source.py} class handles the acquisition of video streams from various sources,
                                    including live cameras and recorded media. It is responsible for configuring camera settings, initializing video capture, and video frame cropping to adapted to screen size. This component ensures the reliability and stability of video feed intake.

                              \item \textit{Model Loading:} Managed by \emph{model\_loader.py}, this component is important for the model loading and initializing. It loads ".stl" model files from external files, sets up texture mapping, and prepares the models for real-time overlay. The class also checks for errors in model data to prevent crashes or rendering issues during operation. It also optimizes the structures of model data to enhance rendering efficiency and reduce memory overhead.

                              \item \textit{Model Overlay:} The \emph{overlay\_window.py} is central to integrating 3D models with live or recorded video. With the help of VTK, this module could set up a multi-layered rendering environment where each layer can independently handle elements like video backgrounds, 3D models, or some GUI overlays(In the future, maybe.).

                              \item \textit{Transform Management:} The \emph{transform\_manager.py} class provides a method for managing 4x4 transformation matrices crucial for spatial adjustments of models in 3D space. It stores and retrieves transformations efficiently. And allow it for dynamic modifications of object orientations and positions.

                              \item \textit{ArUco Marker Tracking:} Functionality provided by \emph{arucotracker.py} includes detecting and decoding ArUco markers from the video stream using OpenCV. This module calculates position and orientation of detected marker, and handle the spatial position data for model tracking.
                        \end{itemize}


                  \item \textbf{Additional Component - ArUco Marker Generator}
                        \\An independent component in the system is the ArUco Marker Generator, managed by \emph{Aruco\_Generator.py}.
                        This tool allows users to select and visualize different ArUco markers. Users can also save these markers as separate image files.
            \end{enumerate}
            \paragraph{Organization:\\}
            The system’s architecture is designed to easy maintenance and scalability.
            The modular nature of the helper classes allows for isolated development and testing,
            which enhances the system's robustness and flexibility.
            This organization simplifies development and testing
            and enables the integration of additional functionalities in the future with minimal disruption to the existing system.

      \item \textbf{Data Processing and Tracking Algorithms}
            % 详细说明系统所使用的数据结构和算法。
            \begin{enumerate}
                  \item \textbf{Pre-processing for video capture and upload}

                  \item \textbf{Feature Data Structures}
                        % 详细说明用于存储从视频中提取的特征（如ArUco标记的位置、角点检测等）的数据结构，可能涉及到空间哈希表或树结构（如KD树、四叉树）。

                  \item \textbf{Image Processing Algorithms in Multi-Layer Video Rendering}
                        % 描述如何存储和处理连续的图像帧，例如使用循环缓冲区或队列来管理实时视频帧。
                        \\ In a video rendering system, video frames need to be dynamically managed to create complex visual effects, such as models and real-time video overlays in this project. This also involved real-time adjustments to the \gls{alpha}\cite{Alpha} and video. A greyscale image with alpha blending of the RGBA stream was used to precisely control transparency and layering effects to enable the superimposition of a layer's frame (e.g. the model) onto the original frame\cite{9979846}. Ensuring overlay accuracy and visual fidelity is crucial for applications such as augmented reality\cite{SETTIMI2022104272}.
                        \\
                        In addition, it is necessary to update and align the video frames to the appropriate layers by adjusting the data range according to the size of the incoming video frames. This incorporation of real-time processing improves the continuity of the video image by preventing visual interruptions caused by frame misalignment\cite{Wang}.
                        \\
                        I also introduced \gls{aesl} into the multi-layer video rendering system (\emph{Overlay\_and\_Tracking.py}). This enhances image processing, such as when processing video streams involving complex dynamic scenes\cite{7410724}. By dynamically adjusting its smoothing parameter (Alpha), \gls{aes} is able to more accurately adapt to changes in content within a video frame, such as lighting adjustments, scene switching, or object movement, and can reduce visual jitter and blurring due to rapid changes \cite{7410724}.
                        \\
                        Compared with the traditional \gls{emal}, the adaptive feature of AES has a greater advantage. \gls{ema}, although fast in processing and low in computational cost, may not be able to adequately adapt its fixed smoothing parameters to real-time changes in the video content in the face of complex scene variations, thus affecting the final image quality\cite{aes,aes2,InvestopediaEMA}.
                        In a multilayer rendering system, combining AES for real-time video transmission and dynamically adjusting the smoothing parameters according to the content differences between the previous and previous frames can maintain the continuity and naturalness of the visual effects, especially when dealing with moving objects and changing backgrounds. In addition, AES's also better handles scenes with large lighting variations, maintaining the balance of colors and shades of light and dark \cite{7298776}.
                        \\
                        I have similarly experimented with \gls{cesl}, and while it excels in handling data with clear trends and cyclical variations, its application in video rendering systems may not be as straightforward and effective as AES. Because \gls{ces} is designed to provide a more comprehensive understanding of the multiple influences on the data \cite{ComplexES2018,Complex}, its use in non-predictive applications may lead to overly complex processing and increased computational burden.
                        \\
                        Therefore, AES is ultimately used in video rendering systems to respond more directly to real-time changes in video content, reduce visual jitter, and improve the viewing experience.
                        \\
                        To summarize, AES can improve image stability and visual quality, as well as enhance the system's responsiveness to environmental changes. By intelligently adjusting processing parameters, AES helps to ensure high efficiency while also adapting to visual jitter or lighting changes that may be encountered with ArUco marker tracking.
                        \paragraph{Code for the AES:}
                        \begin{minted}[frame=single, linenos=true, fontsize=\footnotesize]{python}
def adjust_alpha(self, new_transform):
      """
      Adjusts the smoothing factor alpha based on the difference 
      between the new transform and the current transform to 
      better adapt to recent data changes.
                                    
      Parameters:
      new_transform (float): The new data point used to 
      update the transform.
      """
      if self.transform is not None:
      # Calculate the absolute difference between the current 
      # and new transforms
      error = abs(new_transform - self.transform)
      # Dynamically adjust alpha based on the error, 
      # inversely scaling it
      self.alpha = max(self.min_alpha, min(self.max_alpha, 
      1 / (1 + error)))
                        
def update(self, new_transform):
      """
      Updates the current transform with a new data point using
      adaptive exponential smoothing.
                                    
      Parameters:
      new_transform (float): The new data point to incorporate
      into the smoothed data.
                                    
      Returns:
      float: The updated transform value.
      """
      if self.transform is None:
      # If no transform has been set yet, initialize it 
      # with the new transform
      self.transform = new_transform
      else:
      # Adjust alpha based on the new data point
      self.adjust_alpha(new_transform)
      # Apply the adjusted alpha to compute 
      # the new smoothed transform
      self.transform = self.alpha * new_transform + 
      (1 - self.alpha) * self.transform
      return self.transform
                              \end{minted}
                  \item \textbf{Marker Detection and Tracking}
                        % 详述用于检测和追踪ArUco标记的算法，包括图像分割、模式识别和机器学习技术。

                  \item \textbf{Model Positioning and Rendering}
                        % 介绍如何将三维模型精确地放置在虚拟环境中，可能涉及计算几何和物体识别算法。

                  \item \textbf{Optimization Techniques}
                        % 如果涉及到性能瓶颈，可以介绍用于优化上述算法的技术，如多线程处理、GPU加速或使用更高效的算法（如快速傅里叶变换代替传统卷积）。

                  \item \textbf{Model Color change}
                        % 分析关键算法的时间和空间复杂度，讨论在不同操作环境（如不同硬件或软件平台）下的性能表现。

                  \item \textbf{ArUco Generator}

            \end{enumerate}


      \item \textbf{User Interface Design}
            \begin{enumerate}
                  \item \textbf{Main Menu(Overlay and Tracking)}


                  \item \textbf{ArUco Generator}


                  \item \textbf{Screen Mockups, Sketches, and Screenshots}


            \end{enumerate}


      \item \textbf{Design Notation and Diagrams}
            \begin{enumerate}
                  \item \textbf{Use Case Diagrams}
                        % 描述用例图及其在设计中的应用。

                  \item \textbf{Interaction Diagrams}
                        % 介绍交互图和类图的设计和功能。

                  \item \textbf{Data Flow Diagrams}
                        % 提供系统关键部分的伪代码及数据流图。
            \end{enumerate}
\end{enumerate}

\subsubsection{Implementation}
% 在这里描述 Part1: Real-world Model Interaction and Tracking 部分的具体实现细节。

\subsection{Part2: Endovascular Intervention Simulation}
% 在这里详细介绍关于“Endovascular Intervention Simulation”的设计和实现。

\subsubsection{Design}
% 描述“Endovascular Intervention Simulation”部分的设计细节。

\subsubsection{Implementation}
% 描述“Endovascular Intervention Simulation”部分的具体实现细节。


% Testing & Evaluation
\section{Testing \& Evaluation}

% Project Ethics
\section{Project Ethics}
I have read and abide by the University’s ethical guidelines\cite{UoL_COMP390_2023-24}. The project did not involve direct interaction with human
participants during the design, implementation or evaluation phases. An extensive review of the project scope and methodology
confirmed that no personal data was collected, analyzed or used. In addition, all activities were within the scope of activities
permitted by our ethical guidelines. It was verified with the project supervisor that no customized activities required separate
ethical approval. Therefore, there are no other ethical issues involved in this project.
% Conclusion & Future Work
\section{Conclusion \& Future Work}
\subsection{Conclusion}

\subsection{Future Work}

\section{BCS Criteria \& Self-Reflection}

\subsection{An Ability to Apply Practical and Analytical Skills}
% Describe how you applied practical and analytical skills gained during your degree programme to your project.

\subsection{Innovation and/or Creativity}
% Discuss any innovative or creative approaches employed in your project.

\subsection{Synthesis of Information, Ideas, and Practices}
% Explain how you synthesized information, ideas, and practices to provide a quality solution. Include an evaluation of that solution.

\subsection{Meeting a Real Need in a Wider Context}
% Reflect on how your project meets a real need in a wider context.

\subsection{An Ability to Self-Manage a Significant Piece of Work}
% Reflect on your ability to self-manage the significant piece of work that is your project.

\subsection{Critical Self-Evaluation of the Process}
% Provide a critical self-evaluation of the process involved in your project.


% References (The bibliography will be printed here)
\printbibliography
\printglossaries

\end{document}
